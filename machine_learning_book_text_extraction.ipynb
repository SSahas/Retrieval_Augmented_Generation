{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 sentence-transformers  -Uqqq"
      ],
      "metadata": {
        "id": "xzvxVR_u0yEu"
      },
      "id": "xzvxVR_u0yEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c87e3f",
      "metadata": {
        "id": "02c87e3f"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d7fa68",
      "metadata": {
        "id": "10d7fa68"
      },
      "outputs": [],
      "source": [
        "reader = PdfReader(\"machine_learning.pdf\")\n",
        "pages = len(reader.pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb28dc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecb28dc2",
        "outputId": "f01fc624-74a1-46a2-93bf-45bc3756427a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7bd929503be0>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fcd930",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5fcd930",
        "outputId": "d4867e92-7131-44de-efb6-4a09a2856b80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04492fa3",
      "metadata": {
        "id": "04492fa3"
      },
      "outputs": [],
      "source": [
        "class TreeNode:\n",
        "    def __init__(self, name, page=None, memory_index= None):\n",
        "        self.name = name\n",
        "        self.page = page\n",
        "        self.memory_index = memory_index\n",
        "        self.children = []\n",
        "        self.sections = 0\n",
        "\n",
        "    def add_child(self, child):\n",
        "        self.children.append(child)\n",
        "        self.sections += 1\n",
        "\n",
        "    def __repr__(self, level=0):\n",
        "        ret = \"\\t\" * level + repr(self.name) + \"\\n\"\n",
        "        for child in self.children:\n",
        "            ret += child.__repr__(level + 1)\n",
        "        return ret\n",
        "\n",
        "    def find_node(self, name):\n",
        "        if self.name == name:\n",
        "            return self\n",
        "        for child in self.children:\n",
        "            result = child.find_node(name)\n",
        "            if result:\n",
        "                return result\n",
        "        return None\n",
        "\n",
        "    def delete_node(self, name):\n",
        "        for child in self.children:\n",
        "            if child.name == name:\n",
        "                self.children.remove(child)\n",
        "                return True\n",
        "            else:\n",
        "                result = child.delete_node(name)\n",
        "                if result:\n",
        "                    return result\n",
        "        return False\n",
        "\n",
        "# Create the root of the tree\n",
        "root = TreeNode(\"Machine learning\")\n",
        "\n",
        "# Create the nodes for the main chapters\n",
        "chapter1 = TreeNode(\"1. Introduction\", 1)\n",
        "chapter2 = TreeNode(\"2. Supervised Learning\", 25)\n",
        "chapter3 = TreeNode(\"3. Unsupervised Learning and Preprocessing\", 131)\n",
        "chapter4 = TreeNode(\"4. Representing Data and Engineering Features\", 211)\n",
        "chapter5 = TreeNode(\"5. Model Evaluation and Improvement\", 251)\n",
        "chapter6 = TreeNode(\"6. Algorithm Chains and Pipelines\", 306)\n",
        "chapter7 = TreeNode(\"7. Working with Text Data\", 323)\n",
        "chapter8 = TreeNode(\"8. Wrapping Up\", 357)\n",
        "\n",
        "# Add chapters to the root\n",
        "root.add_child(chapter1)\n",
        "root.add_child(chapter2)\n",
        "root.add_child(chapter3)\n",
        "root.add_child(chapter4)\n",
        "root.add_child(chapter5)\n",
        "root.add_child(chapter6)\n",
        "root.add_child(chapter7)\n",
        "root.add_child(chapter8)\n",
        "\n",
        "# Add subchapters to chapter 1\n",
        "chapter1.add_child(TreeNode(\"Why Machine Learning?\", 1))\n",
        "chapter1.children[0].add_child(TreeNode(\"Problems Machine Learning Can Solve\", 2))\n",
        "chapter1.children[0].add_child(TreeNode(\"Knowing Your Task and Knowing Your Data\", 4))\n",
        "chapter1.add_child(TreeNode(\"Why Python?\", 5))\n",
        "chapter1.add_child(TreeNode(\"scikit-learn\", 5))\n",
        "chapter1.children[2].add_child(TreeNode(\"Installing scikit-learn\", 6))\n",
        "chapter1.add_child(TreeNode(\"Essential Libraries and Tools\", 7))\n",
        "chapter1.children[3].add_child(TreeNode(\"Jupyter Notebook\", 7))\n",
        "chapter1.children[3].add_child(TreeNode(\"NumPy\", 7))\n",
        "chapter1.children[3].add_child(TreeNode(\"SciPy\", 8))\n",
        "chapter1.children[3].add_child(TreeNode(\"matplotlib\", 9))\n",
        "chapter1.children[3].add_child(TreeNode(\"pandas\", 10))\n",
        "chapter1.children[3].add_child(TreeNode(\"mglearn\", 11))\n",
        "chapter1.add_child(TreeNode(\"Python 2 Versus Python 3\", 12))\n",
        "chapter1.add_child(TreeNode(\"Versions Used in this Book\", 12))\n",
        "chapter1.add_child(TreeNode(\"A First Application: Classifying Iris Species\", 13))\n",
        "chapter1.children[6].add_child(TreeNode(\"Meet the Data\", 14))\n",
        "chapter1.children[6].add_child(TreeNode(\"Measuring Success: Training and Testing Data\", 17))\n",
        "chapter1.children[6].add_child(TreeNode(\"First Things First: Look at Your Data\", 19))\n",
        "chapter1.children[6].add_child(TreeNode(\"Building Your First Model: k-Nearest Neighbors\", 20))\n",
        "chapter1.children[6].add_child(TreeNode(\"Making Predictions\", 22))\n",
        "chapter1.children[6].add_child(TreeNode(\"Evaluating the Model\", 22))\n",
        "chapter1.add_child(TreeNode(\"Summary and Outlook\", 23))\n",
        "\n",
        "# Add subchapters to chapter 2\n",
        "chapter2.add_child(TreeNode(\"Classification and Regression\", 25))\n",
        "chapter2.add_child(TreeNode(\"Generalization, Overfitting, and Underfitting\", 26))\n",
        "chapter2.children[1].add_child(TreeNode(\"Relation of Model Complexity to Dataset Size\", 29))\n",
        "chapter2.add_child(TreeNode(\"Supervised Machine Learning Algorithms\", 29))\n",
        "chapter2.children[2].add_child(TreeNode(\"Some Sample Datasets\", 30))\n",
        "chapter2.children[2].add_child(TreeNode(\"k-Nearest Neighbors\", 35))\n",
        "chapter2.children[2].add_child(TreeNode(\"Linear Models\", 45))\n",
        "chapter2.children[2].add_child(TreeNode(\"Naive Bayes Classifiers\", 68))\n",
        "chapter2.children[2].add_child(TreeNode(\"Decision Trees\", 70))\n",
        "chapter2.children[2].add_child(TreeNode(\"Ensembles of Decision Trees\", 83))\n",
        "chapter2.children[2].add_child(TreeNode(\"Kernelized Support Vector Machines\", 92))\n",
        "chapter2.children[2].add_child(TreeNode(\"Neural Networks (Deep Learning)\", 104))\n",
        "chapter2.add_child(TreeNode(\"Uncertainty Estimates from Classifiers\", 119))\n",
        "chapter2.children[3].add_child(TreeNode(\"The Decision Function\", 120))\n",
        "chapter2.children[3].add_child(TreeNode(\"Predicting Probabilities\", 122))\n",
        "chapter2.children[3].add_child(TreeNode(\"Uncertainty in Multiclass Classification\", 124))\n",
        "chapter2.add_child(TreeNode(\"Summary and Outlook\", 127))\n",
        "\n",
        "# Add subchapters to chapter 3\n",
        "chapter3.add_child(TreeNode(\"Types of Unsupervised Learning\", 131))\n",
        "chapter3.add_child(TreeNode(\"Challenges in Unsupervised Learning\", 132))\n",
        "chapter3.add_child(TreeNode(\"Preprocessing and Scaling\", 132))\n",
        "chapter3.children[2].add_child(TreeNode(\"Different Kinds of Preprocessing\", 133))\n",
        "chapter3.children[2].add_child(TreeNode(\"Applying Data Transformations\", 134))\n",
        "chapter3.children[2].add_child(TreeNode(\"Scaling Training and Test Data the Same Way\", 136))\n",
        "chapter3.children[2].add_child(TreeNode(\"The Effect of Preprocessing on Supervised Learning\", 138))\n",
        "chapter3.add_child(TreeNode(\"Dimensionality Reduction, Feature Extraction, and Manifold Learning\", 140))\n",
        "chapter3.children[3].add_child(TreeNode(\"Principal Component Analysis (PCA)\", 140))\n",
        "chapter3.children[3].add_child(TreeNode(\"Non-Negative Matrix Factorization (NMF)\", 156))\n",
        "chapter3.children[3].add_child(TreeNode(\"Manifold Learning with t-SNE\", 163))\n",
        "chapter3.add_child(TreeNode(\"Clustering\", 168))\n",
        "chapter3.children[4].add_child(TreeNode(\"k-Means Clustering\", 168))\n",
        "chapter3.children[4].add_child(TreeNode(\"Agglomerative Clustering\", 182))\n",
        "chapter3.children[4].add_child(TreeNode(\"DBSCAN\", 187))\n",
        "chapter3.children[4].add_child(TreeNode(\"Comparing and Evaluating Clustering Algorithms\", 191))\n",
        "chapter3.children[4].add_child(TreeNode(\"Summary of Clustering Methods\", 207))\n",
        "chapter3.add_child(TreeNode(\"Summary and Outlook\", 208))\n",
        "\n",
        "# Add subchapters to chapter 4\n",
        "chapter4.add_child(TreeNode(\"Categorical Variables\", 212))\n",
        "chapter4.children[0].add_child(TreeNode(\"One-Hot-Encoding (Dummy Variables)\", 213))\n",
        "chapter4.children[0].add_child(TreeNode(\"Numbers Can Encode Categoricals\", 218))\n",
        "chapter4.add_child(TreeNode(\"Binning, Discretization, Linear Models, and Trees\", 220))\n",
        "chapter4.add_child(TreeNode(\"Interactions and Polynomials\", 224))\n",
        "chapter4.add_child(TreeNode(\"Univariate Nonlinear Transformations\", 232))\n",
        "chapter4.add_child(TreeNode(\"Automatic Feature Selection\", 236))\n",
        "chapter4.children[4].add_child(TreeNode(\"Univariate Statistics\", 236))\n",
        "chapter4.children[4].add_child(TreeNode(\"Model-Based Feature Selection\", 238))\n",
        "chapter4.children[4].add_child(TreeNode(\"Iterative Feature Selection\", 240))\n",
        "chapter4.add_child(TreeNode(\"Utilizing Expert Knowledge\", 242))\n",
        "chapter4.add_child(TreeNode(\"Summary and Outlook\", 250))\n",
        "\n",
        "# Add subchapters to chapter 5\n",
        "chapter5.add_child(TreeNode(\"Cross-Validation\", 252))\n",
        "chapter5.children[0].add_child(TreeNode(\"Cross-Validation in scikit-learn\", 253))\n",
        "chapter5.children[0].add_child(TreeNode(\"Benefits of Cross-Validation\", 254))\n",
        "chapter5.children[0].add_child(TreeNode(\"Stratified k-Fold Cross-Validation and Other Strategies\", 254))\n",
        "chapter5.add_child(TreeNode(\"Grid Search\", 260))\n",
        "chapter5.children[1].add_child(TreeNode(\"Simple Grid Search\", 261))\n",
        "chapter5.children[1].add_child(TreeNode(\"The Danger of Overfitting the Parameters and the Validation Set\", 261))\n",
        "chapter5.children[1].add_child(TreeNode(\"Grid Search with Cross-Validation\", 263))\n",
        "chapter5.add_child(TreeNode(\"Evaluation Metrics and Scoring\", 275))\n",
        "chapter5.children[2].add_child(TreeNode(\"Keep the End Goal in Mind\", 275))\n",
        "chapter5.children[2].add_child(TreeNode(\"Metrics for Binary Classification\", 276))\n",
        "chapter5.children[2].add_child(TreeNode(\"Metrics for Multiclass Classification\", 296))\n",
        "chapter5.children[2].add_child(TreeNode(\"Regression Metrics\", 299))\n",
        "chapter5.children[2].add_child(TreeNode(\"Using Evaluation Metrics in Model Selection\", 300))\n",
        "chapter5.add_child(TreeNode(\"Summary and Outlook\", 302))\n",
        "\n",
        "# Add subchapters to chapter 6\n",
        "chapter6.add_child(TreeNode(\"Parameter Selection with Preprocessing\", 306))\n",
        "chapter6.add_child(TreeNode(\"Building Pipelines\", 308))\n",
        "chapter6.add_child(TreeNode(\"Using Pipelines in Grid Searches\", 309))\n",
        "chapter6.add_child(TreeNode(\"The General Pipeline Interface\", 312))\n",
        "chapter6.children[3].add_child(TreeNode(\"Convenient Pipeline Creation with make_pipeline\", 313))\n",
        "chapter6.children[3].add_child(TreeNode(\"Accessing Step Attributes\", 314))\n",
        "chapter6.children[3].add_child(TreeNode(\"Accessing Attributes in a Grid-Searched Pipeline\", 315))\n",
        "chapter6.add_child(TreeNode(\"Grid-Searching Preprocessing Steps and Model Parameters\", 317))\n",
        "chapter6.add_child(TreeNode(\"Grid-Searching Which Model To Use\", 319))\n",
        "chapter6.add_child(TreeNode(\"Summary and Outlook\", 320))\n",
        "\n",
        "# Add subchapters to chapter 7\n",
        "chapter7.add_child(TreeNode(\"Types of Data Represented as Strings\", 323))\n",
        "chapter7.add_child(TreeNode(\"Example Application: Sentiment Analysis of Movie Reviews\", 325))\n",
        "chapter7.add_child(TreeNode(\"Representing Text Data as a Bag of Words\", 327))\n",
        "chapter7.children[2].add_child(TreeNode(\"Applying Bag-of-Words to a Toy Dataset\", 329))\n",
        "chapter7.children[2].add_child(TreeNode(\"Bag-of-Words for Movie Reviews\", 330))\n",
        "chapter7.add_child(TreeNode(\"Stopwords\", 334))\n",
        "chapter7.add_child(TreeNode(\"Rescaling the Data with tf–idf\", 336))\n",
        "chapter7.add_child(TreeNode(\"Investigating Model Coefficients\", 338))\n",
        "chapter7.add_child(TreeNode(\"Bag-of-Words with More Than One Word (n-Grams)\", 339))\n",
        "chapter7.add_child(TreeNode(\"Advanced Tokenization, Stemming, and Lemmatization\", 344))\n",
        "chapter7.add_child(TreeNode(\"Topic Modeling and Document Clustering\", 347))\n",
        "chapter7.children[8].add_child(TreeNode(\"Latent Dirichlet Allocation\", 348))\n",
        "chapter7.add_child(TreeNode(\"Summary and Outlook\", 355))\n",
        "\n",
        "# Add subchapters to chapter 8\n",
        "chapter8.add_child(TreeNode(\"Approaching a Machine Learning Problem\", 357))\n",
        "chapter8.children[0].add_child(TreeNode(\"Humans in the Loop\", 358))\n",
        "chapter8.add_child(TreeNode(\"From Prototype to Production\", 359))\n",
        "chapter8.add_child(TreeNode(\"Testing Production Systems\", 359))\n",
        "chapter8.add_child(TreeNode(\"Building Your Own Estimator\", 360))\n",
        "chapter8.add_child(TreeNode(\"Where to Go from Here\", 361))\n",
        "chapter8.children[4].add_child(TreeNode(\"Theory\", 361))\n",
        "chapter8.children[4].add_child(TreeNode(\"Other Machine Learning Frameworks and Packages\", 362))\n",
        "chapter8.children[4].add_child(TreeNode(\"Ranking, Recommender Systems, and Other Kinds of Learning\", 363))\n",
        "chapter8.children[4].add_child(TreeNode(\"Probabilistic Modeling, Inference, and Probabilistic Programming\", 363))\n",
        "chapter8.children[4].add_child(TreeNode(\"Neural Networks\", 364))\n",
        "chapter8.children[4].add_child(TreeNode(\"Scaling to Larger Datasets\", 364))\n",
        "chapter8.children[4].add_child(TreeNode(\"Honing Your Skills\", 365))\n",
        "chapter8.add_child(TreeNode(\"Conclusion\", 365))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479d5a64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "479d5a64",
        "outputId": "ede47e82-3039-4ddd-b53b-1c588d0e7bc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning'\n",
              "\t'1. Introduction'\n",
              "\t\t'Why Machine Learning?'\n",
              "\t\t\t'Problems Machine Learning Can Solve'\n",
              "\t\t\t'Knowing Your Task and Knowing Your Data'\n",
              "\t\t'Why Python?'\n",
              "\t\t'scikit-learn'\n",
              "\t\t\t'Installing scikit-learn'\n",
              "\t\t'Essential Libraries and Tools'\n",
              "\t\t\t'Jupyter Notebook'\n",
              "\t\t\t'NumPy'\n",
              "\t\t\t'SciPy'\n",
              "\t\t\t'matplotlib'\n",
              "\t\t\t'pandas'\n",
              "\t\t\t'mglearn'\n",
              "\t\t'Python 2 Versus Python 3'\n",
              "\t\t'Versions Used in this Book'\n",
              "\t\t'A First Application: Classifying Iris Species'\n",
              "\t\t\t'Meet the Data'\n",
              "\t\t\t'Measuring Success: Training and Testing Data'\n",
              "\t\t\t'First Things First: Look at Your Data'\n",
              "\t\t\t'Building Your First Model: k-Nearest Neighbors'\n",
              "\t\t\t'Making Predictions'\n",
              "\t\t\t'Evaluating the Model'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'2. Supervised Learning'\n",
              "\t\t'Classification and Regression'\n",
              "\t\t'Generalization, Overfitting, and Underfitting'\n",
              "\t\t\t'Relation of Model Complexity to Dataset Size'\n",
              "\t\t'Supervised Machine Learning Algorithms'\n",
              "\t\t\t'Some Sample Datasets'\n",
              "\t\t\t'k-Nearest Neighbors'\n",
              "\t\t\t'Linear Models'\n",
              "\t\t\t'Naive Bayes Classifiers'\n",
              "\t\t\t'Decision Trees'\n",
              "\t\t\t'Ensembles of Decision Trees'\n",
              "\t\t\t'Kernelized Support Vector Machines'\n",
              "\t\t\t'Neural Networks (Deep Learning)'\n",
              "\t\t'Uncertainty Estimates from Classifiers'\n",
              "\t\t\t'The Decision Function'\n",
              "\t\t\t'Predicting Probabilities'\n",
              "\t\t\t'Uncertainty in Multiclass Classification'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'3. Unsupervised Learning and Preprocessing'\n",
              "\t\t'Types of Unsupervised Learning'\n",
              "\t\t'Challenges in Unsupervised Learning'\n",
              "\t\t'Preprocessing and Scaling'\n",
              "\t\t\t'Different Kinds of Preprocessing'\n",
              "\t\t\t'Applying Data Transformations'\n",
              "\t\t\t'Scaling Training and Test Data the Same Way'\n",
              "\t\t\t'The Effect of Preprocessing on Supervised Learning'\n",
              "\t\t'Dimensionality Reduction, Feature Extraction, and Manifold Learning'\n",
              "\t\t\t'Principal Component Analysis (PCA)'\n",
              "\t\t\t'Non-Negative Matrix Factorization (NMF)'\n",
              "\t\t\t'Manifold Learning with t-SNE'\n",
              "\t\t'Clustering'\n",
              "\t\t\t'k-Means Clustering'\n",
              "\t\t\t'Agglomerative Clustering'\n",
              "\t\t\t'DBSCAN'\n",
              "\t\t\t'Comparing and Evaluating Clustering Algorithms'\n",
              "\t\t\t'Summary of Clustering Methods'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'4. Representing Data and Engineering Features'\n",
              "\t\t'Categorical Variables'\n",
              "\t\t\t'One-Hot-Encoding (Dummy Variables)'\n",
              "\t\t\t'Numbers Can Encode Categoricals'\n",
              "\t\t'Binning, Discretization, Linear Models, and Trees'\n",
              "\t\t'Interactions and Polynomials'\n",
              "\t\t'Univariate Nonlinear Transformations'\n",
              "\t\t'Automatic Feature Selection'\n",
              "\t\t\t'Univariate Statistics'\n",
              "\t\t\t'Model-Based Feature Selection'\n",
              "\t\t\t'Iterative Feature Selection'\n",
              "\t\t'Utilizing Expert Knowledge'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'5. Model Evaluation and Improvement'\n",
              "\t\t'Cross-Validation'\n",
              "\t\t\t'Cross-Validation in scikit-learn'\n",
              "\t\t\t'Benefits of Cross-Validation'\n",
              "\t\t\t'Stratified k-Fold Cross-Validation and Other Strategies'\n",
              "\t\t'Grid Search'\n",
              "\t\t\t'Simple Grid Search'\n",
              "\t\t\t'The Danger of Overfitting the Parameters and the Validation Set'\n",
              "\t\t\t'Grid Search with Cross-Validation'\n",
              "\t\t'Evaluation Metrics and Scoring'\n",
              "\t\t\t'Keep the End Goal in Mind'\n",
              "\t\t\t'Metrics for Binary Classification'\n",
              "\t\t\t'Metrics for Multiclass Classification'\n",
              "\t\t\t'Regression Metrics'\n",
              "\t\t\t'Using Evaluation Metrics in Model Selection'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'6. Algorithm Chains and Pipelines'\n",
              "\t\t'Parameter Selection with Preprocessing'\n",
              "\t\t'Building Pipelines'\n",
              "\t\t'Using Pipelines in Grid Searches'\n",
              "\t\t'The General Pipeline Interface'\n",
              "\t\t\t'Convenient Pipeline Creation with make_pipeline'\n",
              "\t\t\t'Accessing Step Attributes'\n",
              "\t\t\t'Accessing Attributes in a Grid-Searched Pipeline'\n",
              "\t\t'Grid-Searching Preprocessing Steps and Model Parameters'\n",
              "\t\t'Grid-Searching Which Model To Use'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'7. Working with Text Data'\n",
              "\t\t'Types of Data Represented as Strings'\n",
              "\t\t'Example Application: Sentiment Analysis of Movie Reviews'\n",
              "\t\t'Representing Text Data as a Bag of Words'\n",
              "\t\t\t'Applying Bag-of-Words to a Toy Dataset'\n",
              "\t\t\t'Bag-of-Words for Movie Reviews'\n",
              "\t\t'Stopwords'\n",
              "\t\t'Rescaling the Data with tf–idf'\n",
              "\t\t'Investigating Model Coefficients'\n",
              "\t\t'Bag-of-Words with More Than One Word (n-Grams)'\n",
              "\t\t'Advanced Tokenization, Stemming, and Lemmatization'\n",
              "\t\t'Topic Modeling and Document Clustering'\n",
              "\t\t\t'Latent Dirichlet Allocation'\n",
              "\t\t'Summary and Outlook'\n",
              "\t'8. Wrapping Up'\n",
              "\t\t'Approaching a Machine Learning Problem'\n",
              "\t\t\t'Humans in the Loop'\n",
              "\t\t'From Prototype to Production'\n",
              "\t\t'Testing Production Systems'\n",
              "\t\t'Building Your Own Estimator'\n",
              "\t\t'Where to Go from Here'\n",
              "\t\t\t'Theory'\n",
              "\t\t\t'Other Machine Learning Frameworks and Packages'\n",
              "\t\t\t'Ranking, Recommender Systems, and Other Kinds of Learning'\n",
              "\t\t\t'Probabilistic Modeling, Inference, and Probabilistic Programming'\n",
              "\t\t\t'Neural Networks'\n",
              "\t\t\t'Scaling to Larger Datasets'\n",
              "\t\t\t'Honing Your Skills'\n",
              "\t\t'Conclusion'"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777733a1",
      "metadata": {
        "id": "777733a1",
        "outputId": "27044887-66d1-41e7-d46e-1e9f17c9487b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "root.children[0].sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ac7d12",
      "metadata": {
        "id": "67ac7d12"
      },
      "outputs": [],
      "source": [
        "textbook_summary = \"\"\"This book is organized roughly as follows:\n",
        "Chapter 1 introduces the fundamental concepts of machine learning and its\n",
        "applications, and describes the setup we will be using throughout the book.\n",
        "Chapters 2 and 3 describe the actual machine learning algorithms that are most\n",
        "widely used in practice, and discuss their advantages and shortcomings.\n",
        "Chapter 4 discusses the importance of how we represent data that is processed by\n",
        "machine learning, and what aspects of the data to pay attention to.\n",
        "Chapter 5 covers advanced methods for model evaluation and parameter tuning,\n",
        "with a particular focus on cross-validation and grid search.\n",
        "Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\n",
        "ing your workflow.\n",
        "Chapter 7 shows how to apply the methods described in earlier chapters to text\n",
        "data, and introduces some text-specific processing techniques.\n",
        "Chapter 8 offers a high-level overview, and includes references to more advanced\n",
        "topics.\n",
        "While Chapters 2 and 3 provide the actual algorithms, understanding all of these\n",
        "algorithms might not be necessary for a beginner. If you need to build a machine\n",
        "learning system ASAP, we suggest starting with Chapter 1 and the opening sections of\n",
        "Chapter 2, which introduce all the core concepts. You can then skip to “Summary and\n",
        "Outlook” on page 127 in Chapter 2, which includes a list of all the supervised models\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44c9180",
      "metadata": {
        "id": "e44c9180"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f12aaf6",
      "metadata": {
        "id": "2f12aaf6"
      },
      "outputs": [],
      "source": [
        "root.memory_index = textbook_summary"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b1b90d30",
      "metadata": {
        "id": "b1b90d30"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa80f056",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "aa80f056",
        "outputId": "01f83f12-fb3d-4e1c-afcf-d6803124535f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This book is organized roughly as follows:\\nChapter 1 introduces the fundamental concepts of machine learning and its\\napplications, and describes the setup we will be using throughout the book.\\nChapters 2 and 3 describe the actual machine learning algorithms that are most\\nwidely used in practice, and discuss their advantages and shortcomings.\\nChapter 4 discusses the importance of how we represent data that is processed by\\nmachine learning, and what aspects of the data to pay attention to.\\nChapter 5 covers advanced methods for model evaluation and parameter tuning,\\nwith a particular focus on cross-validation and grid search.\\nChapter 6 explains the concept of pipelines for chaining models and encapsulat‐\\ning your workflow.\\nChapter 7 shows how to apply the methods described in earlier chapters to text\\ndata, and introduces some text-specific processing techniques.\\nChapter 8 offers a high-level overview, and includes references to more advanced\\ntopics.\\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\\nalgorithms might not be necessary for a beginner. If you need to build a machine\\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "root.memory_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c6bc85",
      "metadata": {
        "id": "36c6bc85",
        "outputId": "f9940492-04e0-4337-b156-620004ee40d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "root.children[0].page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb47f2a5",
      "metadata": {
        "id": "fb47f2a5"
      },
      "outputs": [],
      "source": [
        "def get_text(b, e):\n",
        "\n",
        "    text = ''\n",
        "    for i in range(b-1, e - 1):\n",
        "        page = reader.pages[i]\n",
        "        print(i)\n",
        "        page_text = page.extract_text()\n",
        "        text = text + page_text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "396e639c",
      "metadata": {
        "id": "396e639c",
        "outputId": "b34b4330-709c-4ee1-8083-9604c09d2b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "node = root.find_node(\"2. Supervised Learning\")\n",
        "node.find_node(\"Summary and Outlook\").page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "327be10a",
      "metadata": {
        "id": "327be10a",
        "outputId": "02bb8773-1fa4-40f8-d0e9-0574bd203f1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "root.children[1].find_node(\"Summary and Outlook\").page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f22ed8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f22ed8",
        "outputId": "3bbc2a12-46ab-4b37-c633-2083e1b52675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chp 0 : start 37, end_page 39\n",
            "36\n",
            "37\n",
            "chp 1 : start 141, end_page 145\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "chp 2 : start 222, end_page 225\n",
            "221\n",
            "222\n",
            "223\n",
            "chp 3 : start 264, end_page 265\n",
            "263\n",
            "chp 4 : start 316, end_page 320\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "chp 5 : start 334, end_page 337\n",
            "333\n",
            "334\n",
            "335\n",
            "chp 6 : start 369, end_page 371\n",
            "368\n",
            "369\n",
            "chp 7 : start 371, end_page 372\n",
            "370\n"
          ]
        }
      ],
      "source": [
        "for i in range(root.sections):\n",
        "    if i != root.sections - 1:\n",
        "        start_page = root.children[i].find_node(\"Summary and Outlook\").page\n",
        "        end_page = root.children[i+1].page\n",
        "    else:\n",
        "        start_page = 357\n",
        "        end_page = 358\n",
        "    #print(f\"chp {i} : start {start_page + 14}, end_page {end_page + 14}\")\n",
        "\n",
        "    root.children[i].memory_index = get_text(start_page + 14, end_page + 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdff93d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdff93d8",
        "outputId": "7de922b0-34b1-435e-c5b1-8310d259ae8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In[29]:\n",
            "y_pred = knn.predict(X_test)\n",
            "print(\"Test set predictions: \\n {}\".format(y_pred))\n",
            "Out[29]:\n",
            "Test set predictions:\n",
            " [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\n",
            "In[30]:\n",
            "print(\"Test set score: {:.2f}\" .format(np.mean(y_pred == y_test)))\n",
            "Out[30]:\n",
            "Test set score: 0.97\n",
            "We can also use the score  method of the knn object, which will compute the test set\n",
            "accuracy for us:\n",
            "In[31]:\n",
            "print(\"Test set score: {:.2f}\" .format(knn.score(X_test, y_test)))\n",
            "Out[31]:\n",
            "Test set score: 0.97\n",
            "For this model, the test set accuracy is about 0.97, which means we made the right\n",
            "prediction for 97% of the irises in the test set. Under some mathematical assump‐\n",
            "tions, this means that we can expect our model to be correct 97% of the time for new\n",
            "irises. For our hobby botanist application, this high level of accuracy means that our\n",
            "model may be trustworthy enough to use. In later chapters we will discuss how we\n",
            "can improve performance, and what caveats there are in tuning a model.\n",
            "Summary and Outlook\n",
            "Let’s summarize what we learned in this chapter. We started with a brief introduction\n",
            "to machine learning and its applications, then discussed the distinction between\n",
            "supervised and unsupervised learning and gave an overview of the tools we’ll be\n",
            "using in this book. Then, we formulated the task of predicting which species of iris a\n",
            "particular flower belongs to by using physical measurements of the flower. We used a\n",
            "dataset of measurements that was annotated by an expert with the correct species to\n",
            "build our model, making this a supervised learning task. There were three possible\n",
            "species, setosa , versicolor , or virginica , which made the task a three-class classification\n",
            "problem. The possible species are called classes  in the classification problem, and the\n",
            "species of a single iris is called its label .\n",
            "The Iris dataset consists of two NumPy arrays: one containing the data, which is\n",
            "referred to as X in scikit-learn , and one containing the correct or desired outputs,\n",
            "Summary and Outlook | 23which is called y. The array X is a two-dimensional array of features, with one row per\n",
            "data point and one column per feature. The array y is a one-dimensional array, which\n",
            "here contains one class label, an integer ranging from 0 to 2, for each of the samples.\n",
            "We split our dataset into a training set , to build our model, and a test set , to evaluate\n",
            "how well our model will generalize to new, previously unseen data.We chose the k-nearest neighbors classification algorithm, which makes predictions\n",
            "for a new data point by considering its closest neighbor(s) in the training set. This is\n",
            "implemented in the KNeighborsClassifier  class, which contains the algorithm that\n",
            "builds the model as well as the algorithm that makes a prediction using the model.\n",
            "We instantiated the class, setting parameters. Then we built the model by calling the\n",
            "fit method, passing the training data ( X_train ) and training outputs ( y_train ) as\n",
            "parameters. We evaluated the model using the score  method, which computes the\n",
            "accuracy of the model. We applied the score  method to the test set data and the test\n",
            "set labels and found that our model is about 97% accurate, meaning it is correct 97%\n",
            "of the time on the test set.This gave us the confidence to apply the model to new data (in our example, new\n",
            "flower measurements) and trust that the model will be correct about 97% of the time.Here is a summary of the code needed for the whole training and evaluation\n",
            "procedure:\n",
            "In[32]:\n",
            "X_train, X_test, y_train, y_test = train_test_split (\n",
            "    iris_dataset ['data'], iris_dataset ['target' ], random_state =0)\n",
            "knn = KNeighborsClassifier (n_neighbors =1)\n",
            "knn.fit(X_train, y_train)\n",
            "print(\"Test set score: {:.2f}\" .format(knn.score(X_test, y_test)))\n",
            "Out[32]:\n",
            "Test set score: 0.97\n",
            "This snippet contains the core code for applying any machine learning algorithm\n",
            "using scikit-learn . The fit, predict , and score  methods are the common inter‐\n",
            "face to supervised models in scikit-learn , and with the concepts introduced in this\n",
            "chapter, you can apply these models to many machine learning tasks. In the next\n",
            "chapter, we will go into more depth about the different kinds of supervised models in\n",
            "scikit-learn  and how to apply them successfully.\n",
            "24 | Chapter 1: Introduction\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In[120]:\n",
            "logreg = LogisticRegression ()\n",
            "# represent each target by its class name in the iris dataset\n",
            "named_target  = iris.target_names [y_train]\n",
            "logreg.fit(X_train, named_target )\n",
            "print(\"unique classes in training data: {}\" .format(logreg.classes_ ))\n",
            "print(\"predictions: {}\" .format(logreg.predict(X_test)[:10]))\n",
            "argmax_dec_func  = np.argmax(logreg.decision_function (X_test), axis=1)\n",
            "print(\"argmax of decision function: {}\" .format(argmax_dec_func [:10]))\n",
            "print(\"argmax combined with classes_: {}\" .format(\n",
            "        logreg.classes_ [argmax_dec_func ][:10]))\n",
            "Out[120]:\n",
            "unique classes in training data: ['setosa' 'versicolor' 'virginica']predictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']argmax of decision function: [1 0 2 1 1 0 1 2 1 1]argmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\n",
            "Summary and Outlook\n",
            "We started this chapter with a discussion of model complexity, then discussed gener‐\n",
            "alization , or learning a model that is able to perform well on new, previously unseen\n",
            "data. This led us to the concepts of underfitting, which describes a model that cannot\n",
            "capture the variations present in the training data, and overfitting, which describes a\n",
            "model that focuses too much on the training data and is not able to generalize to new\n",
            "data very well.\n",
            "We then discussed a wide array of machine learning models for classification and\n",
            "regression, what their advantages and disadvantages are, and how to control model\n",
            "complexity for each of them. We saw that for many of the algorithms, setting the right\n",
            "parameters is important for good performance. Some of the algorithms are also sensi‐\n",
            "tive to how we represent the input data, and in particular to how the features are\n",
            "scaled. Therefore, blindly applying an algorithm to a dataset without understanding\n",
            "the assumptions the model makes and the meanings of the parameter settings will\n",
            "rarely lead to an accurate model.This chapter contains a lot of information about the algorithms, and it is not neces‐\n",
            "sary for you to remember all of these details for the following chapters. However,\n",
            "some knowledge of the models described here—and which to use in a specific situa‐\n",
            "tion—is important for successfully applying machine learning in practice. Here is a\n",
            "quick summary of when to use each model:\n",
            "Summary and Outlook | 127Nearest neighbors\n",
            "For small datasets, good as a baseline, easy to explain.\n",
            "Linear models\n",
            "Go-to as a first algorithm to try, good for very large datasets, good for very high-\n",
            "dimensional data.\n",
            "Naive Bayes\n",
            "Only for classification. Even faster than linear models, good for very large data‐\n",
            "sets and high-dimensional data. Often less accurate than linear models.\n",
            "Decision trees\n",
            "Very fast, don’t need scaling of the data, can be visualized and easily explained.\n",
            "Random forests\n",
            "Nearly always perform better than a single decision tree, very robust and power‐\n",
            "ful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\n",
            "Gradient boosted decision trees\n",
            "Often slightly more accurate than random forests. Slower to train but faster to\n",
            "predict than random forests, and smaller in memory. Need more parameter tun‐\n",
            "ing than random forests.\n",
            "Support vector machines\n",
            "Powerful for medium-sized datasets of features with similar meaning. Require\n",
            "scaling of data, sensitive to parameters.\n",
            "Neural networks\n",
            "Can build very complex models, particularly for large datasets. Sensitive to scal‐\n",
            "ing of the data and to the choice of parameters. Large models need a long time to\n",
            "train.\n",
            "When working with a new dataset, it is in general a good idea to start with a simple\n",
            "model, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\n",
            "how far you can get. After understanding more about the data, you can consider\n",
            "moving to an algorithm that can build more complex models, such as random forests,\n",
            "gradient boosted decision trees, SVMs, or neural networks.\n",
            "Y ou should now be in a position where you have some idea of how to apply, tune, and\n",
            "analyze the models we discussed here. In this chapter, we focused on the binary clas‐\n",
            "sification case, as this is usually easiest to understand. Most of the algorithms presen‐\n",
            "ted have classification and regression variants, however, and all of the classification\n",
            "algorithms support both binary and multiclass classification. Try applying any of\n",
            "these algorithms to the built-in datasets in scikit-learn , like the boston_housing  or\n",
            "diabetes  datasets for regression, or the digits  dataset for multiclass classification.\n",
            "Playing around with the algorithms on different datasets will give you a better feel for\n",
            "128 | Chapter 2: Supervised Learninghow long they need to train, how easy it is to analyze the models, and how sensitive\n",
            "they are to the representation of the data.\n",
            "While we analyzed the consequences of different parameter settings for the algo‐\n",
            "rithms we investigated, building a model that actually generalizes well to new data in\n",
            "production is a bit trickier than that. We will see how to properly adjust parameters\n",
            "and how to find good parameters automatically in Chapter 6 .\n",
            "First, though, we will dive in more detail into unsupervised learning and preprocess‐\n",
            "ing in the next chapter.\n",
            "Summary and Outlook | 129\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary and Outlook\n",
            "This chapter introduced a range of unsupervised learning algorithms that can be\n",
            "applied for exploratory data analysis and preprocessing. Having the right representa‐\n",
            "tion of the data is often crucial for supervised or unsupervised learning to succeed,\n",
            "and preprocessing and decomposition methods play an important part in data prepa‐\n",
            "ration.\n",
            "Decomposition, manifold learning, and clustering are essential tools to further your\n",
            "understanding of your data, and can be the only ways to make sense of your data in\n",
            "the absence of supervision information. Even in a supervised setting, exploratory\n",
            "tools are important for a better understanding of the properties of the data. Often it is\n",
            "hard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\n",
            "deter you from using them to gather insights from your data. With these methods\n",
            "under your belt, you are now equipped with all the essential learning algorithms that\n",
            "machine learning practitioners use every day.We encourage you to try clustering and decomposition methods both on two-\n",
            "dimensional toy data and on real-world datasets included in scikit-learn , like the\n",
            "digits , iris , and cancer  datasets.\n",
            "208 | Chapter 3: Unsupervised Learning and PreprocessingSummary of the Estimator Interface\n",
            "Let’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\n",
            "scikit-learn , whether preprocessing, supervised learning, or unsupervised learning\n",
            "algorithms, are implemented as classes. These classes are called estimators  in scikit-\n",
            "learn . To apply an algorithm, you first have to instantiate an object of the particular\n",
            "class:\n",
            "In[87]:\n",
            "from sklearn.linear_model  import LogisticRegression\n",
            "logreg = LogisticRegression ()\n",
            "The estimator class contains the algorithm, and also stores the model that is learned\n",
            "from data using the algorithm.\n",
            "Y ou should set any parameters of the model when constructing the model object.\n",
            "These parameters include regularization, complexity control, number of clusters to\n",
            "find, etc. All estimators have a fit method, which is used to build the model. The fit\n",
            "method always requires as its first argument the data X, represented as a NumPy array\n",
            "or a SciPy sparse matrix, where each row represents a single data point. The data X is\n",
            "always assumed to be a NumPy array or SciPy sparse matrix that has continuous(floating-point) entries. Supervised algorithms also require a y argument, which is a\n",
            "one-dimensional NumPy array containing target values for regression or classifica‐\n",
            "tion (i.e., the known output labels or responses).\n",
            "There are two main ways to apply a learned model in scikit-learn . To create a pre‐\n",
            "diction in the form of a new output like y, you use the predict  method. To create a\n",
            "new representation of the input data X, you use the transform  method. Table 3-1\n",
            "summarizes the use cases of the predict  and transform  methods.\n",
            "Table 3-1. scikit-learn API summary\n",
            "estimator.fit(x_train, [y_train])\n",
            "estimator.predict(X_text) estimator.transform(X_test)\n",
            "Classification Preprocessing\n",
            "Regression Dimensionality reduction\n",
            "Clustering Feature extraction\n",
            " Feature selection\n",
            "Additionally, all supervised models have a score(X_test, y_test)  method that\n",
            "allows an evaluation of the model. In Table 3-1 , X_train  and y_train  refer to the\n",
            "training data and training labels, while X_test  and y_test  refer to the test data and\n",
            "test labels (if applicable).\n",
            "Summary and Outlook | 209\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 4-19. Coefficients  of the linear regression model using a product of hour and day\n",
            "Summary and Outlook\n",
            "In this chapter, we discussed how to deal with different data types (in particular, with\n",
            "categorical variables). We emphasized the importance of representing data in a way\n",
            "that is suitable for the machine learning algorithm—for example, by one-hot-\n",
            "encoding categorical variables. We also discussed the importance of engineering new\n",
            "features, and the possibility of utilizing expert knowledge in creating derived features\n",
            "from your data. In particular, linear models might benefit greatly from generating\n",
            "new features via binning and adding polynomials and interactions, while more com‐\n",
            "plex, nonlinear models like random forests and SVMs might be able to learn more\n",
            "complex tasks without explicitly expanding the feature space. In practice, the features\n",
            "that are used (and the match between features and method) is often the most impor‐\n",
            "tant piece in making a machine learning approach work well.\n",
            "Now that you have a good idea of how to represent your data in an appropriate way\n",
            "and which algorithm to use for which task, the next chapter will focus on evaluating\n",
            "the performance of machine learning models and selecting the right parameter\n",
            "settings.\n",
            "250 | Chapter 4: Representing Data and Engineering Features\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business  (O’Reilly) for more\n",
            "information on this topic.In[70]:\n",
            "from sklearn.metrics.scorer  import SCORERS\n",
            "print(\"Available scorers: \\n{}\".format(sorted(SCORERS.keys())))\n",
            "Out[70]:\n",
            "Available scorers:\n",
            "['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\n",
            "Summary and Outlook\n",
            "In this chapter we discussed cross-validation, grid search, and evaluation metrics, the\n",
            "cornerstones of evaluating and improving machine learning algorithms. The tools\n",
            "described in this chapter, together with the algorithms described in Chapters 2 and 3,\n",
            "are the bread and butter of every machine learning practitioner.\n",
            "There are two particular points that we made in this chapter that warrant repeating,\n",
            "because they are often overlooked by new practitioners. The first has to do with\n",
            "cross-validation. Cross-validation or the use of a test set allow us to evaluate a\n",
            "machine learning model as it will perform in the future. However, if we use the test\n",
            "set or cross-validation to select a model or select model parameters, we “use up” the\n",
            "test data, and using the same data to evaluate how well our model will do in the future\n",
            "will lead to overly optimistic estimates. We therefore need to resort to a split into\n",
            "training data for model building, validation data for model and parameter selection,\n",
            "and test data for model evaluation. Instead of a simple split, we can replace each of\n",
            "these splits with cross-validation. The most commonly used form (as described ear‐\n",
            "lier) is a training/test split for evaluation, and using cross-validation on the training\n",
            "set for model and parameter selection.\n",
            "The second point has to do with the importance of the evaluation metric or scoring\n",
            "function used for model selection and model evaluation. The theory of how to make\n",
            "business decisions from the predictions of a machine learning model is somewhat\n",
            "beyond the scope of this book.7 However, it is rarely the case that the end goal of a\n",
            "machine learning task is building a model with a high accuracy. Make sure that the\n",
            "metric you choose to evaluate and select a model for is a good stand-in for what the\n",
            "model will actually be used for. In reality, classification problems rarely have balanced\n",
            "classes, and often false positives and false negatives have very different consequences.\n",
            "302 | Chapter 5: Model Evaluation and ImprovementMake sure you understand what these consequences are, and pick an evaluation met‐\n",
            "ric accordingly.\n",
            "The model evaluation and selection techniques we have described so far are the most\n",
            "important tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\n",
            "described them in this chapter can only be applied to a single supervised model. We\n",
            "have seen before, however, that many models require preprocessing, and that in some\n",
            "applications, like the face recognition example in Chapter 3 , extracting a different\n",
            "representation of the data can be useful. In the next chapter, we will introduce the\n",
            "Pipeline  class, which allows us to use grid search and cross-validation on these com‐\n",
            "plex chains of algorithms.\n",
            "Summary and Outlook | 303CHAPTER 6\n",
            "Algorithm Chains and Pipelines\n",
            "For many machine learning algorithms, the particular representation of the data that\n",
            "you provide is very important, as we discussed in Chapter 4 . This starts with scaling\n",
            "the data and combining features by hand and goes all the way to learning features\n",
            "using unsupervised machine learning, as we saw in Chapter 3 . Consequently, most\n",
            "machine learning applications require not only the application of a single algorithm,\n",
            "but the chaining together of many different processing steps and machine learning\n",
            "models. In this chapter, we will cover how to use the Pipeline  class to simplify the\n",
            "process of building chains of transformations and models. In particular, we will see\n",
            "how we can combine Pipeline  and GridSearchCV  to search over parameters for all\n",
            "processing steps at once.\n",
            "As an example of the importance of chaining models, we noticed that we can greatly\n",
            "improve the performance of a kernel SVM on the cancer  dataset by using the Min\n",
            "MaxScaler  for preprocessing. Here’s code for splitting the data, computing the mini‐\n",
            "mum and maximum, scaling the data, and training the SVM:\n",
            "In[1]:\n",
            "from sklearn.svm  import SVC\n",
            "from sklearn.datasets  import load_breast_cancer\n",
            "from sklearn.model_selection  import train_test_split\n",
            "from sklearn.preprocessing  import MinMaxScaler\n",
            "# load and split the data\n",
            "cancer = load_breast_cancer ()\n",
            "X_train, X_test, y_train, y_test = train_test_split (\n",
            "    cancer.data, cancer.target, random_state =0)\n",
            "# compute minimum and maximum on the training datascaler = MinMaxScaler ().fit(X_train)\n",
            "305\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Now we can instantiate and run the grid search as usual, here on the cancer  dataset:\n",
            "In[36]:\n",
            "X_train, X_test, y_train, y_test = train_test_split (\n",
            "    cancer.data, cancer.target, random_state =0)\n",
            "grid = GridSearchCV (pipe, param_grid , cv=5)\n",
            "grid.fit(X_train, y_train)\n",
            "print(\"Best params: \\n{}\\n\".format(grid.best_params_ ))\n",
            "print(\"Best cross-validation score: {:.2f}\" .format(grid.best_score_ ))\n",
            "print(\"Test-set score: {:.2f}\" .format(grid.score(X_test, y_test)))\n",
            "Out[36]:\n",
            "Best params:\n",
            "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,     decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',     max_iter=-1, probability=False, random_state=None, shrinking=True,     tol=0.001, verbose=False), 'preprocessing': StandardScaler(copy=True, with_mean=True, with_std=True), 'classifier__C': 10, 'classifier__gamma': 0.01}\n",
            "Best cross-validation score: 0.99\n",
            "Test-set score: 0.98\n",
            "The outcome of the grid search is that SVC with StandardScaler  preprocessing, C=10 ,\n",
            "and gamma=0.01  gave the best result.\n",
            "Summary and Outlook\n",
            "In this chapter we introduced the Pipeline  class, a general-purpose tool to chain\n",
            "together multiple processing steps in a machine learning workflow. Real-world appli‐\n",
            "cations of machine learning rarely involve an isolated use of a model, and instead are\n",
            "a sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\n",
            "into a single Python object that adheres to the familiar scikit-learn  interface of fit,\n",
            "predict , and transform . In particular when doing model evaluation using cross-\n",
            "validation and parameter selection using grid search, using the Pipeline  class to cap‐\n",
            "ture all the processing steps is essential for proper evaluation. The Pipeline  class also\n",
            "allows writing more succinct code, and reduces the likelihood of mistakes that can\n",
            "happen when building processing chains without the pipeline  class (like forgetting\n",
            "to apply all transformers on the test set, or not applying them in the right order).\n",
            "Choosing the right combination of feature extraction, preprocessing, and models is\n",
            "somewhat of an art, and often requires some trial and error. However, using pipe‐\n",
            "lines, this “trying out” of many different processing steps is quite simple. When\n",
            "320 | Chapter 6: Algorithm Chains and Pipelinesexperimenting, be careful not to overcomplicate your processes, and make sure to\n",
            "evaluate whether every component you are including in your model is necessary.\n",
            "With this chapter, we have completed our survey of general-purpose tools and algo‐\n",
            "rithms provided by scikit-learn . Y ou now possess all the required skills and know\n",
            "the necessary mechanisms to apply machine learning in practice. In the next chapter,\n",
            "we will dive in more detail into one particular type of data that is commonly seen in\n",
            "practice, and that requires some special expertise to handle correctly: text data.\n",
            "Summary and Outlook | 321\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "different outcomes. While identifying topics can be helpful, any conclusions you\n",
            "draw from an unsupervised model should be taken with a grain of salt, and we rec‐\n",
            "ommend verifying your intuition by looking at the documents in a specific topic. The\n",
            "topics produced by the LDA.transform  method can also sometimes be used as a com‐\n",
            "pact representation for supervised learning. This is particularly helpful when few\n",
            "training examples are available.\n",
            "Summary and Outlook\n",
            "In this chapter we talked about the basics of processing text, also known as natural\n",
            "language processing  (NLP), with an example application classifying movie reviews.\n",
            "The tools discussed here should serve as a great starting point when trying to process\n",
            "text data. In particular for text classification tasks such as spam and fraud detection\n",
            "or sentiment analysis, bag-of-words representations provide a simple and powerful\n",
            "solution. As is often the case in machine learning, the representation of the data is key\n",
            "in NLP applications, and inspecting the tokens and n-grams that are extracted can\n",
            "give powerful insights into the modeling process. In text-processing applications, it is\n",
            "often possible to introspect models in a meaningful way, as we saw in this chapter, for\n",
            "both supervised and unsupervised tasks. Y ou should take full advantage of this ability\n",
            "when using NLP-based methods in practice.\n",
            "Natural language and text processing is a large research field, and discussing the\n",
            "details of advanced methods is far beyond the scope of this book. If you want to learn\n",
            "more, we recommend the O’Reilly book Natural Language Processing with Python  by\n",
            "Steven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\n",
            "together with an introduction to the nltk  Python package for NLP . Another great and\n",
            "more conceptual book is the standard reference Introduction to Information Retrieval\n",
            "by Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze, which describes\n",
            "fundamental algorithms in information retrieval, NLP , and machine learning. Both\n",
            "books have online versions that can be accessed free of charge. As we discussed ear‐\n",
            "lier, the classes CountVectorizer  and TfidfVectorizer  only implement relatively\n",
            "simple text-processing methods. For more advanced text-processing methods, we\n",
            "recommend the Python packages spacy  (a relatively new but very efficient and well-\n",
            "designed package), nltk  (a very well-established and complete but somewhat dated\n",
            "library), and gensim  (an NLP package with an emphasis on topic modeling).\n",
            "There have been several very exciting new developments in text processing in recent\n",
            "years, which are outside of the scope of this book and relate to neural networks. The\n",
            "first is the use of continuous vector representations, also known as word vectors or\n",
            "distributed word representations, as implemented in the word2vec  library. The origi‐\n",
            "nal paper “Distributed Representations of Words and Phrases and Their Composi‐\n",
            "tionality”  by Thomas Mikolov et al. is a great introduction to the subject. Both spacy\n",
            "Summary and Outlook | 355and gensim  provide functionality for the techniques discussed in this paper and its\n",
            "follow-ups.\n",
            "Another direction in NLP that has picked up momentum in recent years is the use of\n",
            "recurrent neural networks  (RNNs) for text processing. RNNs are a particularly power‐\n",
            "ful type of neural network that can produce output that is again text, in contrast to\n",
            "classification models that can only assign class labels. The ability to produce text as\n",
            "output makes RNNs well suited for automatic translation and summarization. An\n",
            "introduction to the topic can be found in the relatively technical paper “Sequence to\n",
            "Sequence Learning with Neural Networks”  by Ilya Suskever, Oriol Vinyals, and Quoc\n",
            "Le. A more practical tutorial using the tensorflow  framework can be found on the\n",
            "TensorFlow website .\n",
            "356 | Chapter 7: Working with Text Data\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER 8\n",
            "Wrapping Up\n",
            "Y ou now know how to apply the important machine learning algorithms for super‐\n",
            "vised and unsupervised learning, which allow you to solve a wide variety of machine\n",
            "learning problems. Before we leave you to explore all the possibilities that machine\n",
            "learning offers, we want to give you some final words of advice, point you toward\n",
            "some additional resources, and give you suggestions on how you can further improve\n",
            "your machine learning and data science skills.\n",
            "Approaching a Machine Learning Problem\n",
            "With all the great methods that we introduced in this book now at your fingertips, it\n",
            "may be tempting to jump in and start solving your data-related problem by just run‐\n",
            "ning your favorite algorithm. However, this is not usually a good way to begin your\n",
            "analysis. The machine learning algorithm is usually only a small part of a larger data\n",
            "analysis and decision-making process. To make effective use of machine learning, we\n",
            "need to take a step back and consider the problem at large. First, you should think\n",
            "about what kind of question you want to answer. Do you want to do exploratory anal‐\n",
            "ysis and just see if you find something interesting in the data? Or do you already have\n",
            "a particular goal in mind? Often you will start with a goal, like detecting fraudulent\n",
            "user transactions, making movie recommendations, or finding unknown planets. If\n",
            "you have such a goal, before building a system to achieve it, you should first think\n",
            "about how to define and measure success, and what the impact of a successful solu‐\n",
            "tion would be to your overall business or research goals. Let’s say your goal is fraud\n",
            "detection.\n",
            "357\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(root.sections):\n",
        "    print(root.children[i].memory_index)\n",
        "    print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6d024e",
      "metadata": {
        "id": "ff6d024e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965d0bd8",
      "metadata": {
        "id": "965d0bd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee335a8",
      "metadata": {
        "id": "4ee335a8"
      },
      "outputs": [],
      "source": [
        "def get_text_fromnodes(pages:set):\n",
        "        text = ''\n",
        "        for i in pages:\n",
        "            page = reader.pages[i + 13]\n",
        "            page_text = page.extract_text()\n",
        "            text = text + page_text\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdeb93f1",
      "metadata": {
        "id": "bdeb93f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24fff83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c24fff83",
        "outputId": "059a4e67-a270-4ea6-b7b7-8cdf198b10d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problems Machine Learning Can Solve\n",
            "2\n",
            "Knowing Your Task and Knowing Your Data\n",
            "4\n",
            "Installing scikit-learn\n",
            "6\n",
            "Jupyter Notebook\n",
            "7\n",
            "NumPy\n",
            "7\n",
            "SciPy\n",
            "8\n",
            "matplotlib\n",
            "9\n",
            "pandas\n",
            "10\n",
            "mglearn\n",
            "11\n",
            "Meet the Data\n",
            "14\n",
            "Measuring Success: Training and Testing Data\n",
            "17\n",
            "First Things First: Look at Your Data\n",
            "19\n",
            "Building Your First Model: k-Nearest Neighbors\n",
            "20\n",
            "Making Predictions\n",
            "22\n",
            "Evaluating the Model\n",
            "22\n",
            "Relation of Model Complexity to Dataset Size\n",
            "29\n",
            "Some Sample Datasets\n",
            "30\n",
            "k-Nearest Neighbors\n",
            "35\n",
            "Linear Models\n",
            "45\n",
            "Naive Bayes Classifiers\n",
            "68\n",
            "Decision Trees\n",
            "70\n",
            "Ensembles of Decision Trees\n",
            "83\n",
            "Kernelized Support Vector Machines\n",
            "92\n",
            "Neural Networks (Deep Learning)\n",
            "104\n",
            "The Decision Function\n",
            "120\n",
            "Predicting Probabilities\n",
            "122\n",
            "Uncertainty in Multiclass Classification\n",
            "124\n",
            "Different Kinds of Preprocessing\n",
            "133\n",
            "Applying Data Transformations\n",
            "134\n",
            "Scaling Training and Test Data the Same Way\n",
            "136\n",
            "The Effect of Preprocessing on Supervised Learning\n",
            "138\n",
            "Principal Component Analysis (PCA)\n",
            "140\n",
            "Non-Negative Matrix Factorization (NMF)\n",
            "156\n",
            "Manifold Learning with t-SNE\n",
            "163\n",
            "k-Means Clustering\n",
            "168\n",
            "Agglomerative Clustering\n",
            "182\n",
            "DBSCAN\n",
            "187\n",
            "Comparing and Evaluating Clustering Algorithms\n",
            "191\n",
            "Summary of Clustering Methods\n",
            "207\n",
            "One-Hot-Encoding (Dummy Variables)\n",
            "213\n",
            "Numbers Can Encode Categoricals\n",
            "218\n",
            "Univariate Statistics\n",
            "236\n",
            "Model-Based Feature Selection\n",
            "238\n",
            "Iterative Feature Selection\n",
            "240\n",
            "Cross-Validation in scikit-learn\n",
            "253\n",
            "Benefits of Cross-Validation\n",
            "254\n",
            "Stratified k-Fold Cross-Validation and Other Strategies\n",
            "254\n",
            "Simple Grid Search\n",
            "261\n",
            "The Danger of Overfitting the Parameters and the Validation Set\n",
            "261\n",
            "Grid Search with Cross-Validation\n",
            "263\n",
            "Keep the End Goal in Mind\n",
            "275\n",
            "Metrics for Binary Classification\n",
            "276\n",
            "Metrics for Multiclass Classification\n",
            "296\n",
            "Regression Metrics\n",
            "299\n",
            "Using Evaluation Metrics in Model Selection\n",
            "300\n",
            "Convenient Pipeline Creation with make_pipeline\n",
            "313\n",
            "Accessing Step Attributes\n",
            "314\n",
            "Accessing Attributes in a Grid-Searched Pipeline\n",
            "315\n",
            "Applying Bag-of-Words to a Toy Dataset\n",
            "329\n",
            "Bag-of-Words for Movie Reviews\n",
            "330\n",
            "Latent Dirichlet Allocation\n",
            "348\n",
            "Humans in the Loop\n",
            "358\n",
            "Theory\n",
            "361\n",
            "Other Machine Learning Frameworks and Packages\n",
            "362\n",
            "Ranking, Recommender Systems, and Other Kinds of Learning\n",
            "363\n",
            "Probabilistic Modeling, Inference, and Probabilistic Programming\n",
            "363\n",
            "Neural Networks\n",
            "364\n",
            "Scaling to Larger Datasets\n",
            "364\n",
            "Honing Your Skills\n",
            "365\n"
          ]
        }
      ],
      "source": [
        "for i in range(root.sections):\n",
        "    for k in range(root.children[i].sections):\n",
        "        node = root.children[i].children[k]\n",
        "        pages = []\n",
        "        for y in range(node.sections):\n",
        "            #print(node.children[y].name)\n",
        "            page_number = node.children[y].page\n",
        "            #print(page_number)\n",
        "            #print(page_number + 13)\n",
        "            pages.append(page_number)\n",
        "        node.memory_index = get_text_fromnodes(set(pages))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c2fff5",
      "metadata": {
        "id": "d4c2fff5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cffdc47",
      "metadata": {
        "id": "8cffdc47"
      },
      "outputs": [],
      "source": [
        "def get_text_leaf(b, e):\n",
        "    text = ''\n",
        "    for i in range(b + 13, e + 13):\n",
        "        page = reader.pages[i]\n",
        "        page_text = page.extract_text()\n",
        "        text = text + page_text\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023b4055",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "023b4055",
        "outputId": "949267ef-24b2-47e7-9ce7-4e89aa2f7f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Introduction\n",
            "Why Machine Learning?\n",
            "Problems Machine Learning Can Solve\n",
            "node sections 0\n",
            "here\n",
            "Knowing Your Task and Knowing Your Data\n",
            "node sections 1\n",
            "else\n",
            "1. Introduction\n",
            "Why Python?\n",
            "1. Introduction\n",
            "scikit-learn\n",
            "Installing scikit-learn\n",
            "node sections 0\n",
            "else\n",
            "1. Introduction\n",
            "Essential Libraries and Tools\n",
            "Jupyter Notebook\n",
            "node sections 0\n",
            "here\n",
            "NumPy\n",
            "node sections 1\n",
            "here\n",
            "SciPy\n",
            "node sections 2\n",
            "here\n",
            "matplotlib\n",
            "node sections 3\n",
            "here\n",
            "pandas\n",
            "node sections 4\n",
            "here\n",
            "mglearn\n",
            "node sections 5\n",
            "else\n",
            "1. Introduction\n",
            "Python 2 Versus Python 3\n",
            "1. Introduction\n",
            "Versions Used in this Book\n",
            "1. Introduction\n",
            "A First Application: Classifying Iris Species\n",
            "Meet the Data\n",
            "node sections 0\n",
            "here\n",
            "Measuring Success: Training and Testing Data\n",
            "node sections 1\n",
            "here\n",
            "First Things First: Look at Your Data\n",
            "node sections 2\n",
            "here\n",
            "Building Your First Model: k-Nearest Neighbors\n",
            "node sections 3\n",
            "here\n",
            "Making Predictions\n",
            "node sections 4\n",
            "here\n",
            "Evaluating the Model\n",
            "node sections 5\n",
            "else\n",
            "1. Introduction\n",
            "Summary and Outlook\n",
            "2. Supervised Learning\n",
            "Classification and Regression\n",
            "2. Supervised Learning\n",
            "Generalization, Overfitting, and Underfitting\n",
            "Relation of Model Complexity to Dataset Size\n",
            "node sections 0\n",
            "else\n",
            "2. Supervised Learning\n",
            "Supervised Machine Learning Algorithms\n",
            "Some Sample Datasets\n",
            "node sections 0\n",
            "here\n",
            "k-Nearest Neighbors\n",
            "node sections 1\n",
            "here\n",
            "Linear Models\n",
            "node sections 2\n",
            "here\n",
            "Naive Bayes Classifiers\n",
            "node sections 3\n",
            "here\n",
            "Decision Trees\n",
            "node sections 4\n",
            "here\n",
            "Ensembles of Decision Trees\n",
            "node sections 5\n",
            "here\n",
            "Kernelized Support Vector Machines\n",
            "node sections 6\n",
            "here\n",
            "Neural Networks (Deep Learning)\n",
            "node sections 7\n",
            "else\n",
            "2. Supervised Learning\n",
            "Uncertainty Estimates from Classifiers\n",
            "The Decision Function\n",
            "node sections 0\n",
            "here\n",
            "Predicting Probabilities\n",
            "node sections 1\n",
            "here\n",
            "Uncertainty in Multiclass Classification\n",
            "node sections 2\n",
            "else\n",
            "2. Supervised Learning\n",
            "Summary and Outlook\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Types of Unsupervised Learning\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Challenges in Unsupervised Learning\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Preprocessing and Scaling\n",
            "Different Kinds of Preprocessing\n",
            "node sections 0\n",
            "here\n",
            "Applying Data Transformations\n",
            "node sections 1\n",
            "here\n",
            "Scaling Training and Test Data the Same Way\n",
            "node sections 2\n",
            "here\n",
            "The Effect of Preprocessing on Supervised Learning\n",
            "node sections 3\n",
            "else\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Dimensionality Reduction, Feature Extraction, and Manifold Learning\n",
            "Principal Component Analysis (PCA)\n",
            "node sections 0\n",
            "here\n",
            "Non-Negative Matrix Factorization (NMF)\n",
            "node sections 1\n",
            "here\n",
            "Manifold Learning with t-SNE\n",
            "node sections 2\n",
            "else\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Clustering\n",
            "k-Means Clustering\n",
            "node sections 0\n",
            "here\n",
            "Agglomerative Clustering\n",
            "node sections 1\n",
            "here\n",
            "DBSCAN\n",
            "node sections 2\n",
            "here\n",
            "Comparing and Evaluating Clustering Algorithms\n",
            "node sections 3\n",
            "here\n",
            "Summary of Clustering Methods\n",
            "node sections 4\n",
            "else\n",
            "3. Unsupervised Learning and Preprocessing\n",
            "Summary and Outlook\n",
            "4. Representing Data and Engineering Features\n",
            "Categorical Variables\n",
            "One-Hot-Encoding (Dummy Variables)\n",
            "node sections 0\n",
            "here\n",
            "Numbers Can Encode Categoricals\n",
            "node sections 1\n",
            "else\n",
            "4. Representing Data and Engineering Features\n",
            "Binning, Discretization, Linear Models, and Trees\n",
            "4. Representing Data and Engineering Features\n",
            "Interactions and Polynomials\n",
            "4. Representing Data and Engineering Features\n",
            "Univariate Nonlinear Transformations\n",
            "4. Representing Data and Engineering Features\n",
            "Automatic Feature Selection\n",
            "Univariate Statistics\n",
            "node sections 0\n",
            "here\n",
            "Model-Based Feature Selection\n",
            "node sections 1\n",
            "here\n",
            "Iterative Feature Selection\n",
            "node sections 2\n",
            "else\n",
            "4. Representing Data and Engineering Features\n",
            "Utilizing Expert Knowledge\n",
            "4. Representing Data and Engineering Features\n",
            "Summary and Outlook\n",
            "5. Model Evaluation and Improvement\n",
            "Cross-Validation\n",
            "Cross-Validation in scikit-learn\n",
            "node sections 0\n",
            "here\n",
            "Benefits of Cross-Validation\n",
            "node sections 1\n",
            "here\n",
            "Stratified k-Fold Cross-Validation and Other Strategies\n",
            "node sections 2\n",
            "else\n",
            "5. Model Evaluation and Improvement\n",
            "Grid Search\n",
            "Simple Grid Search\n",
            "node sections 0\n",
            "here\n",
            "The Danger of Overfitting the Parameters and the Validation Set\n",
            "node sections 1\n",
            "here\n",
            "Grid Search with Cross-Validation\n",
            "node sections 2\n",
            "else\n",
            "5. Model Evaluation and Improvement\n",
            "Evaluation Metrics and Scoring\n",
            "Keep the End Goal in Mind\n",
            "node sections 0\n",
            "here\n",
            "Metrics for Binary Classification\n",
            "node sections 1\n",
            "here\n",
            "Metrics for Multiclass Classification\n",
            "node sections 2\n",
            "here\n",
            "Regression Metrics\n",
            "node sections 3\n",
            "here\n",
            "Using Evaluation Metrics in Model Selection\n",
            "node sections 4\n",
            "else\n",
            "5. Model Evaluation and Improvement\n",
            "Summary and Outlook\n",
            "6. Algorithm Chains and Pipelines\n",
            "Parameter Selection with Preprocessing\n",
            "6. Algorithm Chains and Pipelines\n",
            "Building Pipelines\n",
            "6. Algorithm Chains and Pipelines\n",
            "Using Pipelines in Grid Searches\n",
            "6. Algorithm Chains and Pipelines\n",
            "The General Pipeline Interface\n",
            "Convenient Pipeline Creation with make_pipeline\n",
            "node sections 0\n",
            "here\n",
            "Accessing Step Attributes\n",
            "node sections 1\n",
            "here\n",
            "Accessing Attributes in a Grid-Searched Pipeline\n",
            "node sections 2\n",
            "else\n",
            "6. Algorithm Chains and Pipelines\n",
            "Grid-Searching Preprocessing Steps and Model Parameters\n",
            "6. Algorithm Chains and Pipelines\n",
            "Grid-Searching Which Model To Use\n",
            "6. Algorithm Chains and Pipelines\n",
            "Summary and Outlook\n",
            "7. Working with Text Data\n",
            "Types of Data Represented as Strings\n",
            "7. Working with Text Data\n",
            "Example Application: Sentiment Analysis of Movie Reviews\n",
            "7. Working with Text Data\n",
            "Representing Text Data as a Bag of Words\n",
            "Applying Bag-of-Words to a Toy Dataset\n",
            "node sections 0\n",
            "here\n",
            "Bag-of-Words for Movie Reviews\n",
            "node sections 1\n",
            "else\n",
            "7. Working with Text Data\n",
            "Stopwords\n",
            "7. Working with Text Data\n",
            "Rescaling the Data with tf–idf\n",
            "7. Working with Text Data\n",
            "Investigating Model Coefficients\n",
            "7. Working with Text Data\n",
            "Bag-of-Words with More Than One Word (n-Grams)\n",
            "7. Working with Text Data\n",
            "Advanced Tokenization, Stemming, and Lemmatization\n",
            "7. Working with Text Data\n",
            "Topic Modeling and Document Clustering\n",
            "Latent Dirichlet Allocation\n",
            "node sections 0\n",
            "else\n",
            "7. Working with Text Data\n",
            "Summary and Outlook\n",
            "8. Wrapping Up\n",
            "Approaching a Machine Learning Problem\n",
            "Humans in the Loop\n",
            "node sections 0\n",
            "else\n",
            "8. Wrapping Up\n",
            "From Prototype to Production\n",
            "8. Wrapping Up\n",
            "Testing Production Systems\n",
            "8. Wrapping Up\n",
            "Building Your Own Estimator\n",
            "8. Wrapping Up\n",
            "Where to Go from Here\n",
            "Theory\n",
            "node sections 0\n",
            "here\n",
            "Other Machine Learning Frameworks and Packages\n",
            "node sections 1\n",
            "here\n",
            "Ranking, Recommender Systems, and Other Kinds of Learning\n",
            "node sections 2\n",
            "here\n",
            "Probabilistic Modeling, Inference, and Probabilistic Programming\n",
            "node sections 3\n",
            "here\n",
            "Neural Networks\n",
            "node sections 4\n",
            "here\n",
            "Scaling to Larger Datasets\n",
            "node sections 5\n",
            "here\n",
            "Honing Your Skills\n",
            "node sections 6\n",
            "else\n",
            "8. Wrapping Up\n",
            "Conclusion\n"
          ]
        }
      ],
      "source": [
        "for i in range(root.sections):\n",
        "    for k in range(root.children[i].sections):\n",
        "        print(root.children[i].name)\n",
        "        node = root.children[i].children[k]\n",
        "        print(node.name)\n",
        "        if k != root.children[i].sections - 1:\n",
        "            end_page = root.children[i].children[k + 1].page\n",
        "        else:\n",
        "            end_page = node.page\n",
        "        for y in range(node.sections):\n",
        "            print(node.children[y].name)\n",
        "            start_page = node.children[y].page\n",
        "            print(\"node sections\", y)\n",
        "\n",
        "            if y != node.sections-1:\n",
        "                print(\"here\")\n",
        "                nodes_end = node.children[y+1].page\n",
        "            else:\n",
        "                print(\"else\")\n",
        "                nodes_end = end_page\n",
        "\n",
        "            node.children[y].memory_index = get_text_leaf(start_page, nodes_end)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traverse_tree(node, parent_name=None, result=None):\n",
        "    if result is None:\n",
        "        result = []\n",
        "    result.append({\n",
        "        \"parent\": parent_name,\n",
        "        \"current\": node.name,\n",
        "        \"memory_index\": node.memory_index\n",
        "    })\n",
        "    for child in node.children:\n",
        "        traverse_tree(child, node.name, result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kC7xJZ9ZsAS2"
      },
      "id": "kC7xJZ9ZsAS2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_id = []\n",
        "id = []\n",
        "content = []"
      ],
      "metadata": {
        "id": "qnGGLIlisIPW"
      },
      "id": "qnGGLIlisIPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = traverse_tree(root)\n",
        "for record in result:\n",
        "   parent_id.append(record['parent'])\n",
        "   id.append(record['current'])\n",
        "   content.append(record['memory_index'])"
      ],
      "metadata": {
        "id": "8yry3z-lsAPZ"
      },
      "id": "8yry3z-lsAPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(parent_id))\n",
        "print(len(id))\n",
        "print(len(content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7XP0z_AsANC",
        "outputId": "9d38f077-0b07-4e37-bf6e-bc8126967dd1"
      },
      "id": "W7XP0z_AsANC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131\n",
            "131\n",
            "131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dict = {'parent_id': parent_id, 'node_id': id, 'memory': content}\n",
        "\n",
        "df = pd.DataFrame(dict)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noTFR2OmsAKz",
        "outputId": "2e10002e-66c2-476a-c626-11173e954fdd"
      },
      "id": "noTFR2OmsAKz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 parent_id                                            node_id  \\\n",
            "0                     None                                   Machine learning   \n",
            "1         Machine learning                                    1. Introduction   \n",
            "2          1. Introduction                              Why Machine Learning?   \n",
            "3    Why Machine Learning?                Problems Machine Learning Can Solve   \n",
            "4    Why Machine Learning?            Knowing Your Task and Knowing Your Data   \n",
            "..                     ...                                                ...   \n",
            "126  Where to Go from Here  Probabilistic Modeling, Inference, and Probabi...   \n",
            "127  Where to Go from Here                                    Neural Networks   \n",
            "128  Where to Go from Here                         Scaling to Larger Datasets   \n",
            "129  Where to Go from Here                                 Honing Your Skills   \n",
            "130         8. Wrapping Up                                         Conclusion   \n",
            "\n",
            "                                                memory  \n",
            "0    This book is organized roughly as follows:\\nCh...  \n",
            "1    In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"...  \n",
            "2    spam. This would be an example of using an exp...  \n",
            "3    spam. This would be an example of using an exp...  \n",
            "4    Segmenting customers into groups with similar ...  \n",
            "..                                                 ...  \n",
            "126  Ranking, Recommender Systems, and Other Kinds ...  \n",
            "127                                                     \n",
            "128  2A preprint of Deep Learning  can be viewed at...  \n",
            "129                                                     \n",
            "130                                                     \n",
            "\n",
            "[131 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "92-FinE2sAIq",
        "outputId": "1333d78a-f520-4807-a014-ee742884a44f"
      },
      "id": "92-FinE2sAIq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 parent_id                                            node_id  \\\n",
              "0                     None                                   Machine learning   \n",
              "1         Machine learning                                    1. Introduction   \n",
              "2          1. Introduction                              Why Machine Learning?   \n",
              "3    Why Machine Learning?                Problems Machine Learning Can Solve   \n",
              "4    Why Machine Learning?            Knowing Your Task and Knowing Your Data   \n",
              "..                     ...                                                ...   \n",
              "126  Where to Go from Here  Probabilistic Modeling, Inference, and Probabi...   \n",
              "127  Where to Go from Here                                    Neural Networks   \n",
              "128  Where to Go from Here                         Scaling to Larger Datasets   \n",
              "129  Where to Go from Here                                 Honing Your Skills   \n",
              "130         8. Wrapping Up                                         Conclusion   \n",
              "\n",
              "                                                memory  \n",
              "0    This book is organized roughly as follows:\\nCh...  \n",
              "1    In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"...  \n",
              "2    spam. This would be an example of using an exp...  \n",
              "3    spam. This would be an example of using an exp...  \n",
              "4    Segmenting customers into groups with similar ...  \n",
              "..                                                 ...  \n",
              "126  Ranking, Recommender Systems, and Other Kinds ...  \n",
              "127                                                     \n",
              "128  2A preprint of Deep Learning  can be viewed at...  \n",
              "129                                                     \n",
              "130                                                     \n",
              "\n",
              "[131 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-168841f4-58e8-4854-b50f-74505b9481ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_id</th>\n",
              "      <th>node_id</th>\n",
              "      <th>memory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>Machine learning</td>\n",
              "      <td>This book is organized roughly as follows:\\nCh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Machine learning</td>\n",
              "      <td>1. Introduction</td>\n",
              "      <td>In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1. Introduction</td>\n",
              "      <td>Why Machine Learning?</td>\n",
              "      <td>spam. This would be an example of using an exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why Machine Learning?</td>\n",
              "      <td>Problems Machine Learning Can Solve</td>\n",
              "      <td>spam. This would be an example of using an exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why Machine Learning?</td>\n",
              "      <td>Knowing Your Task and Knowing Your Data</td>\n",
              "      <td>Segmenting customers into groups with similar ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Where to Go from Here</td>\n",
              "      <td>Probabilistic Modeling, Inference, and Probabi...</td>\n",
              "      <td>Ranking, Recommender Systems, and Other Kinds ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Where to Go from Here</td>\n",
              "      <td>Neural Networks</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Where to Go from Here</td>\n",
              "      <td>Scaling to Larger Datasets</td>\n",
              "      <td>2A preprint of Deep Learning  can be viewed at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>Where to Go from Here</td>\n",
              "      <td>Honing Your Skills</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>8. Wrapping Up</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-168841f4-58e8-4854-b50f-74505b9481ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-168841f4-58e8-4854-b50f-74505b9481ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-168841f4-58e8-4854-b50f-74505b9481ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8450b285-25c1-4a8c-bfbb-d69be73d1275\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8450b285-25c1-4a8c-bfbb-d69be73d1275')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8450b285-25c1-4a8c-bfbb-d69be73d1275 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6a74d9b0-ac11-4d65-8f53-2601c123b878\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6a74d9b0-ac11-4d65-8f53-2601c123b878 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 131,\n  \"fields\": [\n    {\n      \"column\": \"parent_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Approaching a Machine Learning Problem\",\n          \"Automatic Feature Selection\",\n          \"Dimensionality Reduction, Feature Extraction, and Manifold Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"node_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 125,\n        \"samples\": [\n          \"Meet the Data\",\n          \"3. Unsupervised Learning and Preprocessing\",\n          \"Kernelized Support Vector Machines\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"memory\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          \"The result of t-SNE is quite remarkable. All the classes are quite clearly separated.\\nThe ones and nines are somewhat split up, but most of the classes form a single dense\\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com\\u2010\\npletely unsupervised. Still, it can find a representation of the data in two dimensions\\nthat clearly separates the classes, based solely on how close points are in the original\\nspace.\\nThe t-SNE algorithm has some tuning parameters, though it often works well with\\nthe default settings. Y ou can try playing with perplexity  and early_exaggeration ,\\nbut the effects are usually minor.\\nClustering\\nAs we described earlier, clustering  is the task of partitioning the dataset into groups,\\ncalled clusters. The goal is to split up the data in such a way that points within a single\\ncluster are very similar and points in different clusters are different. Similarly to clas\\u2010\\nsification algorithms, clustering algorithms assign (or predict) a number to each data\\npoint, indicating which cluster a particular point belongs to.\\nk-Means Clustering\\nk-means clustering is one of the simplest and most commonly used clustering algo\\u2010\\nrithms. It tries to find cluster centers  that are representative of certain regions of the\\ndata. The algorithm alternates between two steps: assigning each data point to the\\nclosest cluster center, and then setting each cluster center as the mean of the data\\npoints that are assigned to it. The algorithm is finished when the assignment of\\ninstances to clusters no longer changes. The following example ( Figure 3-23 ) illus\\u2010\\ntrates the algorithm on a synthetic dataset:\\nIn[47]:\\nmglearn.plots.plot_kmeans_algorithm ()\\n168 | Chapter 3: Unsupervised Learning and PreprocessingFigure 3-47. Images from selected clusters found by agglomerative clustering when set\\u2010\\nting the number of clusters to 40\\u2014the text to the left shows the index of the cluster and\\nthe total number of points in the cluster\\nHere, the clustering seems to have picked up on \\u201cdark skinned and smiling, \\u201d \\u201ccollared\\nshirt, \\u201d \\u201csmiling woman, \\u201d \\u201cHussein, \\u201d and \\u201chigh forehead. \\u201d We could also find these\\nhighly similar clusters using the dendrogram, if we did more a detailed analysis.\\nSummary of Clustering Methods\\nThis section has shown that applying and evaluating clustering is a highly qualitative\\nprocedure, and often most helpful in the exploratory phase of data analysis. We\\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster\\u2010\\ning. All three have a way of controlling the granularity of clustering. k-means and\\nagglomerative clustering allow you to specify the number of desired clusters, while\\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ\\u2010\\nences cluster size. All three methods can be used on large, real-world datasets, are rel\\u2010\\natively easy to understand, and allow for clustering into many clusters.\\nEach of the algorithms has somewhat different strengths. k-means allows for a char\\u2010\\nacterization of the clusters using the cluster means. It can also be viewed as a decom\\u2010\\nposition method, where each data point is represented by its cluster center. DBSCAN\\nallows for the detection of \\u201cnoise points\\u201d that are not assigned any cluster, and it can\\nhelp automatically determine the number of clusters. In contrast to the other two\\nmethods, it allow for complex cluster shapes, as we saw in the two_moons  example.\\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\\npartitions of the data, which can be easily inspected via dendrograms.\\nClustering | 207Agglomerative Clustering\\nAgglomerative clustering  refers to a collection of clustering algorithms that all build\\nupon the same principles: the algorithm starts by declaring each point its own cluster,\\nand then merges the two most similar clusters until some stopping criterion is satis\\u2010\\nfied. The stopping criterion implemented in scikit-learn  is the number of clusters,\\nso similar clusters are merged until only the specified number of clusters are left.\\nThere are several linkage  criteria that specify how exactly the \\u201cmost similar cluster\\u201d is\\nmeasured. This measure is always defined between two existing clusters.\\nThe following three choices are implemented in scikit-learn :\\nward\\nThe default choice, ward  picks the two clusters to merge such that the variance\\nwithin all clusters increases the least. This often leads to clusters that are rela\\u2010\\ntively equally sized.\\naverage\\naverage  linkage merges the two clusters that have the smallest average distance\\nbetween all their points.\\ncomplete\\ncomplete  linkage (also known as maximum linkage) merges the two clusters that\\nhave the smallest maximum distance between their points.\\nward  works on most datasets, and we will use it in our examples. If the clusters have\\nvery dissimilar numbers of members (if one is much bigger than all the others, for\\nexample), average  or complete  might work better.\\nThe following plot ( Figure 3-33 ) illustrates the progression of agglomerative cluster\\u2010\\ning on a two-dimensional dataset, looking for three clusters:\\nIn[61]:\\nmglearn.plots.plot_agglomerative_algorithm ()\\n182 | Chapter 3: Unsupervised Learning and PreprocessingThe y-axis in the dendrogram doesn\\u2019t just specify when in the agglomerative algo\\u2010\\nrithm two clusters get merged. The length of each branch also shows how far apart\\nthe merged clusters are. The longest branches in this dendrogram are the three lines\\nthat are marked by the dashed line labeled \\u201cthree clusters. \\u201d That these are the longest\\nbranches indicates that going from three to two clusters meant merging some very\\nfar-apart points. We see this again at the top of the chart, where merging the two\\nremaining clusters into a single cluster again bridges a relatively large distance.\\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\\nthe two_moons  dataset. But the same is not true for the next algorithm we will look at,\\nDBSCAN.\\nDBSCAN\\nAnother very useful clustering algorithm is DBSCAN (which stands for \\u201cdensity-\\nbased spatial clustering of applications with noise\\u201d). The main benefits of DBSCAN\\nare that it does not require the user to set the number of clusters a priori , it can cap\\u2010\\nture clusters of complex shapes, and it can identify points that are not part of any\\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\\nstill scales to relatively large datasets.\\nDBSCAN works by identifying points that are in \\u201ccrowded\\u201d regions of the feature\\nspace, where many data points are close together. These regions are referred to as\\ndense  regions in feature space. The idea behind DBSCAN is that clusters form dense\\nregions of data, separated by regions that are relatively empty.Points that are within a dense region are called core samples  (or core points), and they\\nare defined as follows. There are two parameters in DBSCAN: min_samples  and eps.\\nIf there are at least min_samples  many data points within a distance of eps to a given\\ndata point, that data point is classified as a core sample. Core samples that are closer\\nto each other than the distance eps are put into the same cluster by DBSCAN.\\nThe algorithm works by picking an arbitrary point to start with. It then finds all\\npoints with distance eps or less from that point. If there are less than min_samples\\npoints within distance eps of the starting point, this point is labeled as noise , meaning\\nthat it doesn\\u2019t belong to any cluster. If there are more than min_samples  points within\\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\\nThen, all neighbors (within eps) of the point are visited. If they have not been\\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\\ngrows until there are no more core samples within distance eps of the cluster. Then\\nanother point that hasn\\u2019t yet been visited is picked, and the same procedure is\\nrepeated.\\nClustering | 187Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\\nComparing and Evaluating Clustering Algorithms\\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\\nhow well an algorithm worked, and to compare outcomes between different algo\\u2010\\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\\nand DBSCAN, we will now compare them on some real-world datasets.\\nEvaluating clustering with ground truth\\nThere are metrics that can be used to assess the outcome of a clustering algorithm\\nrelative to a ground truth clustering, the most important ones being the adjusted rand\\nindex  (ARI) and normalized mutual information  (NMI), which both provide a quanti\\u2010\\ntative measure between 0 and 1.\\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\\nusing ARI. We also include what it looks like when we randomly assign points to two\\nclusters for comparison (see Figure 3-39 ):\\nClustering | 191\",\n          \"Numbers Can Encode Categoricals\\nIn the example of the adult  dataset, the categorical variables were encoded as strings.\\nOn the one hand, that opens up the possibility of spelling errors, but on the other\\nhand, it clearly marks a variable as categorical. Often, whether for ease of storage or\\nbecause of the way the data is collected, categorical variables are encoded as integers.\\nFor example, imagine the census data in the adult  dataset was collected using a ques\\u2010\\ntionnaire, and the answers for workclass  were recorded as 0 (first box ticked), 1 (sec\\u2010\\nond box ticked), 2 (third box ticked), and so on. Now the column will contain\\nnumbers from 0 to 8, instead of strings like \\\"Private\\\" , and it won\\u2019t be immediately\\nobvious to someone looking at the table representing the dataset whether they should\\ntreat this variable as continuous or categorical. Knowing that the numbers indicate\\nemployment status, however, it is clear that these are very distinct states and should\\nnot be modeled by a single continuous variable.\\nCategorical features are often encoded using integers. That they are\\nnumbers doesn\\u2019t mean that they should necessarily be treated as\\ncontinuous features. It is not always clear whether an integer fea\\u2010\\nture should be treated as continuous or discrete (and one-hot-\\nencoded). If there is no ordering between the semantics that are\\nencoded (like in the workclass  example), the feature must be\\ntreated as discrete. For other cases, like five-star ratings, the better\\nencoding depends on the particular task and data and which\\nmachine learning algorithm is used.\\nThe get_dummies  function in pandas  treats all numbers as continuous and will not\\ncreate dummy variables for them. To get around this, you can either use scikit-\\nlearn \\u2019s OneHotEncoder , for which you can specify which variables are continuous\\nand which are discrete, or convert numeric columns in the DataFrame  to strings. To\\nillustrate, let\\u2019s create a DataFrame  object with two columns, one containing strings\\nand one containing integers:\\nIn[8]:\\n# create a DataFrame with an integer feature and a categorical string feature\\ndemo_df = pd.DataFrame ({'Integer Feature' : [0, 1, 2, 1],\\n                        'Categorical Feature' : ['socks', 'fox', 'socks', 'box']})\\ndisplay(demo_df)\\nTable 4-4  shows the result.\\n218 | Chapter 4: Representing Data and Engineering FeaturesTable 4-4. DataFrame containing categorical string features and integer features\\nCategorical Feature Integer Feature\\n0 socks 0\\n1 fox 1\\n2 socks 2\\n3 box 1\\nUsing get_dummies  will only encode the string feature and will not change the integer\\nfeature, as you can see in Table 4-5 :\\nIn[9]:\\npd.get_dummies (demo_df)\\nTable 4-5. One-hot-encoded version of the data from Table 4-4 , leaving the integer feature\\nunchanged\\nInteger Feature Categorical Feature_box Categorical Feature_fox Categorical Feature_socks\\n0 0 0.0 0.0 1.0\\n1 1 0.0 1.0 0.0\\n2 2 0.0 0.0 1.0\\n3 1 1.0 0.0 0.0\\nIf you want dummy variables to be created for the \\u201cInteger Feature\\u201d column, you can\\nexplicitly list the columns you want to encode using the columns  parameter. Then,\\nboth features will be treated as categorical (see Table 4-6 ):\\nIn[10]:\\ndemo_df['Integer Feature' ] = demo_df['Integer Feature' ].astype(str)\\npd.get_dummies (demo_df, columns=['Integer Feature' , 'Categorical Feature' ])\\nTable 4-6. One-hot encoding of the data shown in Table 4-4 , encoding the integer and string\\nfeatures\\nInteger\\nFeature_0Integer\\nFeature_1Integer\\nFeature_2Categorical\\nFeature_boxCategorical\\nFeature_foxCategorical\\nFeature_socks\\n0 1.0 0.0 0.0 0.0 0.0 1.0\\n1 0.0 1.0 0.0 0.0 1.0 0.0\\n2 0.0 0.0 1.0 0.0 0.0 1.0\\n3 0.0 1.0 0.0 1.0 0.0 0.0\\nCategorical Variables | 219\",\n          \"The Decision Function\\nIn the binary classification case, the return value of decision_function  is of shape\\n(n_samples,) , and it returns one floating-point number for each sample:\\nIn[106]:\\nprint(\\\"X_test.shape: {}\\\" .format(X_test.shape))\\nprint(\\\"Decision function shape: {}\\\" .format(\\n    gbrt.decision_function (X_test).shape))\\nOut[106]:\\nX_test.shape: (25, 2)\\nDecision function shape: (25,)\\nThis value encodes how strongly the model believes a data point to belong to the\\n\\u201cpositive\\u201d class, in this case class 1. Positive values indicate a preference for the posi\\u2010\\ntive class, and negative values indicate a preference for the \\u201cnegative\\u201d (other) class:\\nIn[107]:\\n# show the first few entries of decision_functionprint(\\\"Decision function: \\\\n{}\\\".format(gbrt.decision_function (X_test)[:6]))\\nOut[107]:\\nDecision function:[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\\nWe can recover the prediction by looking only at the sign of the decision function:\\nIn[108]:\\nprint(\\\"Thresholded decision function: \\\\n{}\\\".format(\\n    gbrt.decision_function (X_test) > 0))\\nprint(\\\"Predictions: \\\\n{}\\\".format(gbrt.predict(X_test)))\\nOut[108]:\\nThresholded decision function:[ True False False False  True  True False  True  True  True False  True  True False  True False False False  True  True  True  True  True False  False]Predictions:['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue' 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red' 'red' 'blue' 'blue']\\nFor binary classification, the \\u201cnegative\\u201d class is always the first entry of the classes_\\nattribute, and the \\u201cpositive\\u201d class is the second entry of classes_ . So if you want to\\nfully recover the output of predict , you need to make use of the classes_  attribute:\\n120 | Chapter 2: Supervised LearningFigure 2-55. Decision boundary (left)  and decision function (right) for a gradient boost\\u2010\\ning model on a two-dimensional toy dataset\\nEncoding not only the predicted outcome but also how certain the classifier is pro\\u2010\\nvides additional information. However, in this visualization, it is hard to make out the\\nboundary between the two classes.\\nPredicting Probabilities\\nThe output of predict_proba  is a probability for each class, and is often more easily\\nunderstood than the output of decision_function . It is always of shape (n_samples,\\n2) for binary classification:\\nIn[112]:\\nprint(\\\"Shape of probabilities: {}\\\" .format(gbrt.predict_proba (X_test).shape))\\nOut[112]:\\nShape of probabilities: (25, 2)\\nThe first entry in each row is the estimated probability of the first class, and the sec\\u2010\\nond entry is the estimated probability of the second class. Because it is a probability,\\nthe output of predict_proba  is always between 0 and 1, and the sum of the entries\\nfor both classes is always 1:\\nIn[113]:\\n# show the first few entries of predict_proba\\nprint(\\\"Predicted probabilities: \\\\n{}\\\".format(\\n    gbrt.predict_proba (X_test[:6])))\\n122 | Chapter 2: Supervised LearningFigure 2-56. Decision boundary (left)  and predicted probabilities for the gradient boost\\u2010\\ning model shown in Figure 2-55\\nThe boundaries in this plot are much more well-defined, and the small areas of\\nuncertainty are clearly visible.\\nThe scikit-learn  website  has a great comparison of many models and what their\\nuncertainty estimates look like. We\\u2019ve reproduced this in Figure 2-57 , and we encour\\u2010\\nage you to go though the example there.\\nFigure 2-57. Comparison of several classifiers  in scikit-learn on synthetic datasets (image\\ncourtesy http://scikit-learn.org)\\nUncertainty in Multiclass Classification\\nSo far, we\\u2019ve only talked about uncertainty estimates in binary classification. But the\\ndecision_function  and predict_proba  methods also work in the multiclass setting.\\nLet\\u2019s apply them on the Iris dataset, which is a three-class classification dataset:\\n124 | Chapter 2: Supervised Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('machine_learning.csv')"
      ],
      "metadata": {
        "id": "01MmxR6CsAGb"
      },
      "id": "01MmxR6CsAGb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eddOIc4fsAED"
      },
      "id": "eddOIc4fsAED",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fcvYgoydsAB0"
      },
      "id": "fcvYgoydsAB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cbf0f4",
      "metadata": {
        "id": "46cbf0f4"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3e1c5c",
      "metadata": {
        "id": "7e3e1c5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FpedvDNr6w8m"
      },
      "id": "FpedvDNr6w8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UcBJJf46w5E"
      },
      "id": "7UcBJJf46w5E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSeuppgK6w2W"
      },
      "id": "LSeuppgK6w2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "5hIgG8j66w0E"
      },
      "id": "5hIgG8j66w0E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-azTgBOr6wxW"
      },
      "id": "-azTgBOr6wxW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4fdf8d",
      "metadata": {
        "id": "6a4fdf8d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbde77d",
      "metadata": {
        "id": "6bbde77d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97449986",
      "metadata": {
        "id": "97449986"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3b8140",
      "metadata": {
        "id": "fb3b8140"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21b1ed5",
      "metadata": {
        "id": "a21b1ed5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5499480",
      "metadata": {
        "id": "e5499480"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aede3314",
      "metadata": {
        "id": "aede3314"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2cf23f",
      "metadata": {
        "id": "2e2cf23f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920742dd",
      "metadata": {
        "id": "920742dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiidQshAIUVv"
      },
      "id": "tiidQshAIUVv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wm1wccxqIUTH"
      },
      "id": "wm1wccxqIUTH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWkzSjJbIUQb"
      },
      "id": "WWkzSjJbIUQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vj6mGA_sIUK-"
      },
      "id": "vj6mGA_sIUK-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}